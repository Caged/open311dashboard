#!/usr/bin/python

import httplib
import urllib2
import urllib
import xml.dom.minidom as dom
import json
import datetime as dt

def get_requests_from_SF(request_ids):
    """
    Retrieve the requests from the San Francisco 311 API within the time range
    specified by the dates start and end.
    
    Returns a stream containing the content from the API call.
    """
    
    url = r'https://open311.sfgov.org/dev/v2/requests.xml'
    query_data = {
        'service_request_id' : ','.join(request_ids),
        'jurisdiction_id' : 'sfgov.org',
    }
    query_str = urllib.urlencode(query_data)
    
    requests_stream = urllib2.urlopen(url + '?' + query_str)
    return requests_stream

def get_open_request_stream_from_couch():
    """
    Retrieve the requests from the San Francisco 311 API within the time range
    specified by the dates start and end.
    
    Returns a stream containing the content from the API call.
    """
    
    url = r'http://open311.couchone.com/service-requests/_design/requests/_view/open'
    query_data = {
    }
    query_str = urllib.urlencode(query_data)
    
    requests_stream = urllib2.urlopen(url + '?' + query_str)
    return requests_stream

def parse_requests_json(stream):
    """
    Converts the given file-like object, which presumably contains a service
    requests document, into a list of request dictionaries.
    """
    import json
    
    requests = json.load(stream)
    
    return requests

def parse_requests_xml(stream):
    """
    Converts the given file-like object, which presumably contains a service
    requests document, into a list of request dictionaries.
    """
    import xml.dom
    
    requests = []
    requests_root = dom.parse(stream).documentElement
    
    for request_node in requests_root.childNodes:
        if request_node.nodeType != xml.dom.Node.ELEMENT_NODE:
            continue
        
        request = {}
        
        if request_node.tagName != 'request':
            raise Exception('Unexpected node: %s' % requests_root.toprettyxml())
        
        for request_attr in request_node.childNodes:
            if request_attr.childNodes:
                key = request_attr.tagName
                value = request_attr.childNodes[0].data
                request[key] = value
        
        request['jurisdiction_id'] = 'sfgov.org'
        requests.append(request)
    
    return requests

def upload_requests_to_couch(requests):
    """
    Send the given service request list to the couchdb.  Return the server 
    response.
    """
    couchdb_host = 'open311.couchone.com'
    couchdb_path = '/service-requests/_bulk_docs'
    docs = {'docs':requests}
    
    couchdb_conn = httplib.HTTPConnection(couchdb_host)
    upload_request = couchdb_conn.request(
        'POST', couchdb_path, json.dumps(docs),
        { 'Content-type' : 'application/json' })
    
    upload_response = couchdb_conn.getresponse()
    return upload_response.read()

def chunk(items, chunk_size):
    """
    Split a list into smaller lists of maximum size chunk_size.
    """
    start_index = 0
    for start_index in xrange(0, len(items), chunk_size):
        end_index = min(start_index+chunk_size, len(items))
        yield items[start_index:end_index]

def find(items, term, key=None):
    """
    Search for an term in a list of items, optionally by a key function.
    """
    if key is None:
        key = lambda other: term == other
    
    for item in items:
        if key(item):
            return item

def get_updates_to_couch_requests(requests):
    # The SF Open311 service can only take a few requests at a time, so chunk
    # the request list and send them in batches.
    request_updates = []
    
    _current = 0
    for requests_chunk in chunk(requests, 10):
        _current += 10
        print "Searching for updated requests up to index %s of %s..." % (_current, len(requests))
        
        request_updates_stream = get_requests_from_SF(
            [request['service_request_id'] 
             for request in requests_chunk]
        )
        requests_chunk_updates = parse_requests_xml(request_updates_stream)
        for request in requests_chunk:
            request_update = find(requests_chunk_updates, request, 
                key=lambda other: request['service_request_id'] == other['service_request_id'])
            
            # You should damn-well have an object in request_update, unless SF
            # deleted the record for the given request.
            assert request_update
            
            request_update['_id'] = request['_id']
            request_update['_rev'] = request['_rev']
            if request_update != request:
                print "Found an updated request:"
                print request
                print request_update
                print
                
                request_updates.append(request_update)
        
        print "There are now %s request(s) to be updated." % len(request_updates)
    
    return request_updates

if __name__ == '__main__':
    import sys
    
#    if len(sys.argv) >= 2:
#        start, end = get_time_range(dt.datetime.strptime(sys.argv[1], '%Y-%m-%d'))
#        print start, end
#    else:
#        start, end = get_time_range()
    
    open_request_stream = get_open_request_stream_from_couch()
    open_request_rows = parse_requests_json(open_request_stream)
    request_updates = get_updates_to_couch_requests([row['value'] for row in open_request_rows['rows']])
    response_json = upload_requests_to_couch(request_updates)
    
    print response_json
